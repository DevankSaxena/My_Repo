GeoTech Data Intelligence System

Overview

This repository contains the design and implementation of the GeoTech Data Intelligence System. The goal of this assignment is to create a system that efficiently collects, processes, stores, and analyzes geospatial data to provide actionable insights for business decision-making. The project leverages various GCP services and technologies to build a scalable, secure, and efficient data intelligence platform.

Table of Contents

System Design

Requirements

Design Document

Technology Stack


Implementation

ETL/ELT Pipelines

API Development


Evaluation Criteria

Bonus Tasks

How to Run the Project

Submission and Contributors



---

System Design

Requirements

1. Data Sources:

The system ingests data from various sources:

Sensor data (e.g., satellite imagery)

Databases

APIs

Files (e.g., GeoJSON)




2. Data Processing:

A robust ETL/ELT pipeline to handle various data formats and perform transformations.



3. Storage:

Scalable and secure storage for both raw and processed data.



4. Data Analysis:

Capabilities for spatial machine learning analysis, OCR, detection, segmentation, visualization, and reporting.

Data quality profiling engines and a data catalog for business interaction.



5. Integration:

Integration with business applications and workflows within GCP.



6. Scalability & Monitoring:

System scalability to handle increasing data volume.

Monitoring and alerting capabilities for system health and performance.



7. APIs:

API catalog for accessing processed data for downstream use cases.




Design Document

The full design document includes:

System Overview:

The architecture includes components such as data sources, processing pipelines (ETL/ELT), data storage, analysis modules, and API services.


Data Flow:

Data flows from sources like GeoJSON files and databases into the ETL/ELT pipeline, where it is processed, stored in GCP services (like BigQuery/Cloud Storage), and made available for analysis and API consumption.


Data Modeling:

Data is structured in a schema optimized for geospatial analysis, leveraging BigQuery's spatial extensions for querying.


Technology Choices:

GCP tools used: Cloud Storage, BigQuery, Dataproc, Dataflow, Cloud Functions, API Gateway.


Security and Privacy:

IAM policies, encryption, and GCPâ€™s security best practices are implemented for data security.


Scalability and Reliability:

The system is designed to scale with growing data volumes and is resilient with high availability through GCP services.


Deployment and Maintenance:

GCP CI/CD tools and monitoring services (Stackdriver) are used for seamless deployment, logging, and alerting.



Technology Stack

Cloud Provider: Google Cloud Platform (GCP)

Data Processing: Dataflow, Dataproc, Cloud Functions

Data Storage: BigQuery, Cloud Storage

APIs: Cloud Endpoints, API Gateway

Monitoring: Stackdriver, Cloud Logging



---

Implementation

ETL/ELT Pipelines

Ingestion Sources:

Data is ingested from a GeoJSON file (states.geojson) and a secondary database/API source.


Processing:

The pipeline reads, transforms, and loads data into GCP storage services (BigQuery, Cloud Storage).


Technologies:

Apache Beam (Dataflow) for transformation and processing.

Cloud Storage for raw data storage.

BigQuery for structured geospatial data storage and querying.



API Development

API Functionality:

A RESTful API is developed that fetches geospatial data from BigQuery, processes it, and returns it as a response.


Deployment:

API is deployed using GCP API Gateway and Cloud Functions for serverless architecture.


Endpoints:

/getGeoData: Fetches geospatial data based on the query parameters.




---

Evaluation Criteria

1. System Design:

Clarity, completeness, feasibility, scalability, and security of the design.



2. Technology Selection:

Relevance and justification of the chosen GCP services for the system.



3. Implementation:

Functionality, code quality, and adherence to best practices.



4. Documentation:

Clarity and comprehensiveness of the design document and code.





---

Bonus Tasks

1. Integration with MLOps:

Demonstration of how the system integrates with MLOps on GCP, such as using AI Platform for model training on geospatial data.



2. Real-world Dataset:

Optional use of real-world datasets in addition to the provided states.geojson file, showcasing insights through analysis.





---

How to Run the Project

1. Fork the Repository:

Fork this repository to your GitHub account.



2. Set up GCP:

Create a free trial GCP account and set up the required services (BigQuery, Cloud Storage, Cloud Functions, API Gateway).



3. Clone the Repository:

git clone <your-forked-repo-url>
cd geotech-data-intelligence


4. Deploy the ETL/ELT Pipelines:

Follow the instructions in the pipeline code to deploy it on GCP.



5. Deploy the API:

Follow the API deployment instructions in the api/README.md to deploy it using GCP services.



6. Run the System:

After deployment, the ETL pipeline will begin ingesting data, and the API will be available at the endpoint for querying.





---


